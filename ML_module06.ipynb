{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYreqbHhWET6FB5afUcbeh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushi1009/ML_assignment-/blob/main/ML_module06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Theoretical**\n",
        "\n",
        "**1. What is Logistic Regression, and how does it differ from Linear Regression.?**\n",
        "\n",
        "Ans-  Logistic Regression :\n",
        "Logistic Regression is used for classification problems, where the output is categorical (e.g., Yes/No, 0/1, Spam/Not Spam). Instead of predicting continuous values like Linear Regression, it estimates the probability of a particular class using the sigmoid (logistic) function.\n",
        "\n",
        "Logistic Regression is used for classification problems, where the output is categorical (e.g., Yes/No, 0/1). It applies the sigmoid function to predict probabilities and uses Log Loss as its cost function. In contrast, Linear Regression is used for regression problems, where the output is continuous (e.g., predicting house prices). It fits a straight-line equation to the data and minimizes Mean Squared Error (MSE). The key difference is that Logistic Regression predicts probabilities for classification, while Linear Regression predicts numerical values for regression.\n",
        "\n",
        "\n",
        "**2. What is the mathematical equation of Logistic Regression.**\n",
        "\n",
        "ANs-\n",
        "The mathematical equation for Logistic Regression is: P(x) = e^(b0 + b1x) / (1 + e^(b0 + b1x))** where \"P(x)\" represents the predicted probability, \"b0\" is the intercept, \"b1\" is the coefficient for the input variable \"x\", and \"e\" is the base of the natural logarithm.\n",
        "\n",
        "**3. Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "ANs-\n",
        "The Sigmoid function is used in Logistic Regression because it maps any real-valued input to a value between 0 and 1, which perfectly aligns with the concept of probability, allowing the model to output the probability of a binary outcome (like \"yes\" or \"no\") based on the input features; essentially transforming the linear combination of features into a probability value that can be interpreted as the likelihood of an event occurring.\n",
        "\n",
        "**4. What is the cost function of Logistic Regression?**\n",
        "\n",
        "Ans-\n",
        "In Logistic Regression, the cost function typically used is called the \"binary cross-entropy loss function\" or simply \"log-loss,\" which essentially measures the average difference between the predicted probabilities and the actual binary labels, aiming to minimize this difference to optimize the model's parameters; it calculates the negative logarithm of the likelihood of the observed data given the model parameters, effectively penalizing the model more for incorrect predictions on the edge of the decision boundary.\n",
        "Key points about the Logistic Regression cost function:\n",
        "\n",
        "Formula:\n",
        "\n",
        "The basic formula for the cost function in Logistic Regression for a single data point (x, y) is:\n",
        "\n",
        "Loss = -[y * log(p) + (1 - y) * log(1 - p)],  \n",
        "where:\n",
        "y is the actual label (0 or 1),\n",
        "p is the predicted probability by the model\n",
        "\n",
        "**5. What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "Ans-\n",
        "\n",
        "In Logistic Regression, \"regularization\" is a technique used to prevent overfitting by adding a penalty term to the cost function, effectively forcing the model to learn less complex relationships between features and the target variable, thus improving its ability to generalize to unseen data; essentially, it helps the model avoid learning too specific patterns from the training data and perform better on new data sets.\n",
        "\n",
        "Why is regularization needed?\n",
        "\n",
        "- Improved generalization:\n",
        "By preventing overfitting, regularization helps the model make more accurate predictions on new data that it hasn't seen before.\n",
        "- Handling high dimensionality:\n",
        "When dealing with a large number of features, regularization can help prevent the model from becoming too complex and overfitting to the training data.\n",
        "\n",
        "**6. Explain the difference between Lasso, Ridge, and Elastic Net regression?**\n",
        "\n",
        "Lasso, Ridge, and Elastic Net are all regression methods that can improve the performance of linear models. They are used to reduce overfitting and improve model predictions.\n",
        "\n",
        "Lasso:\n",
        "Automatically selects important factors and ignores others,\n",
        "Sets some coefficients to zero, which removes unnecessary features,\n",
        "Reduces overfitting in linear models.\n",
        "\n",
        "Ridge:\n",
        "Reduces the impact of features that are not important in predicting y values,\n",
        "Makes all coefficients smaller but doesn't set them to zero,\n",
        "Controls coefficient variance,\n",
        "Stabilizes coefficient estimates.\n",
        "\n",
        "Elastic Net:\n",
        "Combines the strengths of both Lasso and Ridge,\n",
        "Removes some features and reduces others,\n",
        "Balances both feature elimination and coefficient reduction,\n",
        "Takes account of trade-offs between coefficient shrinkage and sparsity control.\n",
        "\n",
        "Each method uses different approaches to solve their optimization problems. Lasso and Elastic Net require an iterative method called coordinate descent.\n",
        "All three methods can be used to improve the performance of linear models, including neural networks.\n",
        "\n",
        "**7. When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "Ans-\n",
        "\n",
        "Elastic Net is preferred over Lasso and Ridge when dealing with **high-dimensional data with many correlated features**. Lasso (L1 regularization) performs **feature selection** by shrinking some coefficients to zero, but it struggles when features are highly correlated, often selecting only one and ignoring the rest. Ridge (L2 regularization), on the other hand, **handles multicollinearity well** by distributing weights among correlated features but does not perform feature selection. Elastic Net **combines both L1 and L2 regularization**, making it ideal when feature selection is needed while still maintaining stability in the presence of multicollinearity. It is especially useful when the number of features is much greater than the number of observations (**p >> n**), where Lasso may struggle. Additionally, Elastic Net prevents Lasso’s tendency to over-penalize and remove too many variables when noise is present. Thus, Elastic Net provides a balance between Lasso’s sparsity and Ridge’s stability, making it the preferred choice when **both feature selection and handling correlated variables** are essential.\n",
        "\n",
        "**8. What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
        "\n",
        "ans-\n",
        "\n",
        "In **Logistic Regression**, the regularization parameter ( lambda ) (often denoted as **C** in some implementations, where ( C = frac{1}{lambda} )) controls the balance between model complexity and overfitting. A **higher ( lambda ) (stronger regularization)** forces the model to shrink coefficients closer to zero, reducing overfitting but potentially underfitting if set too high. A **lower ( lambda ) (weaker regularization)** allows more flexibility, fitting the training data more closely, but may lead to overfitting if the model captures too much noise. Regularization helps improve **generalization** by preventing excessively large weights, especially when dealing with high-dimensional or noisy data. Logistic Regression typically uses **L1 regularization (Lasso) for feature selection** and **L2 regularization (Ridge) for reducing multicollinearity**. Proper tuning of ( lambda ) is crucial to finding the right balance between bias and variance, ensuring the model performs well on unseen data.\n",
        "\n",
        "**9. What are the key assumptions of Logistic Regression?**\n",
        "\n",
        "Ans- Logistic Regression, a popular classification algorithm, operates under several key assumptions. First, it assumes a **linear relationship** between the independent variables and the log-odds (logit) of the dependent variable, rather than a direct linear relationship with the outcome itself. Second, the model requires **independent observations**, meaning that the observations should not be correlated or dependent on each other. Third, **little to no multicollinearity** among the independent variables is essential, as high correlation can distort coefficient estimates. Fourth, Logistic Regression assumes that the dependent variable is **binary** (or can be extended to multinomial or ordinal cases), ensuring that it classifies outcomes correctly. Additionally, it follows the assumption of **large sample size**, particularly when using Maximum Likelihood Estimation (MLE) for parameter estimation, as small samples may lead to unreliable coefficients. Lastly, it assumes that there are **no extreme outliers** in the dataset, as they can disproportionately affect the model’s predictions. While Logistic Regression is flexible and interpretable, violating these assumptions may impact its accuracy and reliability.\n",
        "\n",
        "**10. What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "Ans- While Logistic Regression is a widely used classification algorithm, several alternatives can be more effective depending on the dataset and problem complexity. **Decision Trees** are a popular choice as they work well with both numerical and categorical data, handle non-linear relationships, and are easy to interpret. However, they can be prone to overfitting, which can be mitigated by techniques like pruning.  \n",
        "\n",
        "Another alternative is **Random Forest**, an ensemble method that builds multiple decision trees and combines their outputs for improved accuracy and robustness. It reduces overfitting compared to a single decision tree and performs well on large datasets. Similarly, **Gradient Boosting Machines (GBM)** like XGBoost, LightGBM, and CatBoost are powerful tree-based methods that iteratively improve predictions by focusing on errors from previous models, often outperforming Logistic Regression in complex classification problems.  \n",
        "\n",
        "For datasets with high dimensionality or intricate patterns, **Support Vector Machines (SVM)** can be a strong alternative. SVM finds an optimal hyperplane that maximizes the margin between classes and can handle non-linearly separable data using kernel functions. Meanwhile, **Neural Networks**, particularly deep learning models, excel in cases where the relationships between features and outputs are highly non-linear, such as image or text classification, though they require larger datasets and computational power.  \n",
        "\n",
        "Finally, **k-Nearest Neighbors (k-NN)** provides a simple yet effective approach by classifying data points based on the majority class of their nearest neighbors. It works well when decision boundaries are irregular but can be computationally expensive for large datasets. The choice of classification model depends on factors like data size, interpretability, computation cost, and the nature of the classification problem.\n",
        "\n",
        "**11. What are Classification Evaluation Metrics?**\n",
        "\n",
        "Ans- Classification evaluation metrics help assess how well a model predicts categorical outcomes. **Accuracy** measures overall correctness but can be misleading for imbalanced datasets. **Precision** focuses on the proportion of correctly predicted positives, useful when false positives are costly (e.g., spam detection). **Recall (Sensitivity)** measures how well actual positives are identified, crucial in cases like medical diagnoses. **F1-Score** balances precision and recall, making it useful for imbalanced data.  \n",
        "\n",
        "The **ROC Curve & AUC** evaluate a model's ability to distinguish between classes, while **Log Loss** measures the uncertainty in predicted probabilities. The **Confusion Matrix** provides a detailed breakdown of classification errors. Choosing the right metric depends on the problem—high recall is preferred for fraud detection, while high precision is crucial for email spam classification.\n",
        "\n",
        "**12. How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "Ans- Class imbalance significantly affects **Logistic Regression** by skewing its predictions and reducing model performance. Since Logistic Regression optimizes for **overall accuracy**, it tends to favor the majority class, often predicting it more frequently while ignoring the minority class. This can result in a high accuracy score but poor performance in detecting the minority class, leading to misleading conclusions.  \n",
        "\n",
        "A major issue is that the **decision boundary** may shift towards the majority class, reducing sensitivity (recall) for the minority class. This means the model may fail to identify important instances, such as fraud detection or medical diagnoses, where minority cases are crucial. Additionally, **probability estimates** from Logistic Regression can become biased, making threshold-based classification less reliable.  \n",
        "\n",
        "To address class imbalance, techniques like **resampling (oversampling the minority class or undersampling the majority class)**, using **class-weighted Logistic Regression**, or employing alternative algorithms like **Random Forest, XGBoost, or SMOTE (Synthetic Minority Over-sampling Technique)** can help balance the learning process. Instead of relying solely on accuracy, using metrics like **Precision, Recall, F1-Score, and AUC-ROC** provides a better evaluation of model performance in imbalanced datasets.\n",
        "\n",
        "**13. What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "Ans- **Hyperparameter tuning** in Logistic Regression involves optimizing the model’s hyperparameters to improve its performance. Hyperparameters are settings that are not learned from the data but must be set before training the model. In Logistic Regression, key hyperparameters include **regularization strength (C), penalty type (L1 or L2), solver, and maximum iterations**.  \n",
        "\n",
        "The **regularization parameter (C)** controls the trade-off between model complexity and accuracy. A high C value reduces regularization, allowing the model to fit the training data more closely, while a low C value increases regularization, preventing overfitting. The **penalty type (L1 or L2)** determines whether Lasso (L1) or Ridge (L2) regularization is applied, impacting feature selection and weight shrinkage. The **solver** (e.g., ‘liblinear,’ ‘saga,’ ‘lbfgs’) affects optimization and is chosen based on the dataset size and penalty type. The **maximum iterations** parameter ensures convergence during optimization.  \n",
        "\n",
        "Common techniques for hyperparameter tuning include **Grid Search**, which tests predefined hyperparameter values, and **Random Search**, which randomly selects values from a given range. More advanced methods like **Bayesian Optimization** and **Genetic Algorithms** can also be used for efficient tuning. Proper hyperparameter tuning helps improve model generalization, preventing overfitting or underfitting.\n",
        "\n",
        "**14. What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "Ans- In **Logistic Regression**, solvers are optimization algorithms used to find the best model coefficients. The choice of solver depends on the dataset size, regularization type, and classification problem. The **liblinear** solver is efficient for **small datasets** and supports **L1 and L2 regularization**, making it suitable for binary classification. On the other hand, **lbfgs** works well for **multiclass problems** and supports only **L2 regularization**, making it a good default choice for most cases.  \n",
        "\n",
        "For **large datasets**, **sag** and **saga** are preferred as they are optimized for high-dimensional data. **sag** is faster for **L2 regularization**, while **saga** supports **L1, L2, and Elastic Net regularization**, making it more flexible, especially for sparse datasets. If L1 regularization or Elastic Net is needed, **saga** is the best option.  \n",
        "\n",
        "In summary, **liblinear** is good for small datasets, **lbfgs** for multiclass problems, **sag** for large datasets with L2 regularization, and **saga** for large, sparse datasets requiring L1 or Elastic Net regularization. If unsure, **lbfgs** is a reliable default solver.\n",
        "\n",
        "**15. How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "Ans- Logistic Regression is naturally designed for **binary classification**, but it can be extended to **multiclass classification** using different strategies. One common approach is the **One-vs-Rest (OvR) method**, where a separate **binary classifier** is trained for each class. For a dataset with **K classes**, K different Logistic Regression models are built, each distinguishing one class from all others. During prediction, the class with the highest probability is assigned. This method is simple and works well for most cases, but it can be inefficient when dealing with a large number of classes.  \n",
        "\n",
        "Another approach is **One-vs-One (OvO)**, which trains a separate **binary classifier for each pair of classes**. This means that for K classes, **K(K-1)/2 models** are needed. The final prediction is made using a **voting system**, where the class with the most \"wins\" across all classifiers is chosen. While this method can be effective, it is computationally expensive and rarely used in Logistic Regression.  \n",
        "\n",
        "A more direct method is **Softmax (Multinomial) Regression**, where a single model is trained to predict probabilities for all classes simultaneously. Instead of treating the problem as multiple binary classifications, **Softmax assigns a probability distribution** over all possible classes, selecting the class with the highest probability. This approach is more computationally efficient than OvO and works well for standard multiclass problems. It is implemented in **Scikit-Learn** by setting `multi_class='multinomial'` and works best with solvers like **lbfgs or saga**.  \n",
        "\n",
        "For small datasets, **OvR with liblinear** is a good choice due to its simplicity. For general multiclass classification, **Softmax Regression (multinomial)** is preferred, especially with **lbfgs or saga solvers**. If the dataset is large and has imbalanced classes, **saga with L1 or Elastic Net regularization** can help improve performance. The choice of method depends on dataset size, class distribution, and computational efficiency.\n",
        "\n",
        "**16. What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "Ans- Logistic Regression is a widely used classification algorithm due to its **simplicity, interpretability, and efficiency**. One of its main advantages is that it is **easy to implement and interpret**, making it a great choice for baseline models. It provides **probabilistic predictions**, allowing for confidence estimation in classification tasks. Additionally, it performs well on **linearly separable data** and requires fewer computational resources compared to complex models like neural networks. Logistic Regression also supports **regularization (L1 and L2)**, helping prevent overfitting in high-dimensional datasets.  \n",
        "\n",
        "Despite its advantages, Logistic Regression has several limitations. It **assumes a linear relationship** between independent variables and the log-odds of the dependent variable, which may not always hold true. It is **sensitive to outliers** since extreme values can significantly affect the model’s decision boundary. Logistic Regression struggles with **nonlinear relationships**, requiring feature engineering or transformation techniques to improve performance. Additionally, in cases of **highly imbalanced datasets**, it tends to favor the majority class, making alternative techniques like class weighting or resampling necessary.  \n",
        "\n",
        "Overall, Logistic Regression is a **good starting point for classification problems**, especially when interpretability is important. However, for complex, nonlinear, or imbalanced datasets, more advanced models like **Decision Trees, Random Forest, or Neural Networks** may be more suitable.\n",
        "\n",
        "**17.What are some use cases of Logistic Regression?**\n",
        "\n",
        "Ans- Logistic Regression is widely used in various fields due to its simplicity, interpretability, and effectiveness in classification tasks. One of the most common applications is in **medical diagnosis**, where it helps predict diseases such as diabetes, heart disease, or cancer based on patient data. By analyzing risk factors, it provides probabilistic outputs that assist doctors in decision-making.  \n",
        "\n",
        "In the **financial sector**, Logistic Regression is used for **credit scoring and fraud detection**. Banks and financial institutions use it to determine whether a customer is likely to default on a loan or detect fraudulent transactions based on spending patterns. Its ability to handle binary classification problems makes it a strong candidate for risk assessment.  \n",
        "\n",
        "Another important use case is in **marketing and customer retention**. Businesses use Logistic Regression to predict **customer churn**, helping them identify which customers are likely to leave and take preventive measures. It is also applied in **email spam classification**, distinguishing between spam and non-spam emails by analyzing textual and behavioral features.  \n",
        "\n",
        "In **human resources**, it assists in **employee attrition prediction**, helping companies understand the likelihood of employees leaving based on factors like job satisfaction, workload, and salary. It is also widely used in **political science** to predict **voter behavior**, determining how likely a person is to vote for a particular candidate based on demographic and historical data.  \n",
        "\n",
        "Overall, Logistic Regression is an essential tool in domains where binary or multiclass classification is required, offering a balance between interpretability and performance.\n",
        "\n",
        "**18. What is the difference between Softmax Regression and Logistic Regression?**\n",
        "Ans - **Logistic Regression** and **Softmax Regression** are both classification algorithms, but they differ in how they handle class labels. **Logistic Regression** is specifically designed for **binary classification**, meaning it predicts one of two possible classes. It uses the **sigmoid function** to output probabilities between 0 and 1, where a decision threshold (usually 0.5) is used to classify an instance into one of the two categories. It is simple, interpretable, and works well when the data is **linearly separable**.  \n",
        "\n",
        "On the other hand, **Softmax Regression (Multinomial Logistic Regression)** is an extension of Logistic Regression for **multiclass classification** problems, where there are more than two possible categories. Instead of using the sigmoid function, it uses the **softmax function**, which converts raw scores into a probability distribution across all classes. The class with the highest probability is selected as the final prediction. Softmax Regression ensures that the sum of all class probabilities is **equal to 1**, making it useful when the classes are **mutually exclusive**.  \n",
        "\n",
        "The key difference is that **Logistic Regression is used for binary classification**, while **Softmax Regression is used for multiclass classification**. Logistic Regression can be extended to multiclass classification using the **One-vs-Rest (OvR) approach**, but Softmax Regression provides a more direct and efficient solution for handling multiple classes simultaneously. In **Scikit-Learn**, Softmax Regression is implemented by setting `multi_class='multinomial'` and works best with solvers like **lbfgs or saga**.\n",
        "\n",
        "**19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "ANs- Choosing between **One-vs-Rest (OvR)** and **Softmax (Multinomial Logistic Regression)** for **multiclass classification** depends on several factors, including dataset size, computational efficiency, interpretability, and performance requirements.  \n",
        "\n",
        "**One-vs-Rest (OvR)** is a simple approach where multiple **binary classifiers** are trained—one for each class. It is useful when the dataset is **small or imbalanced**, as it allows individual classifiers to focus on distinguishing one class at a time. OvR works well with solvers like **liblinear**, which is efficient for small datasets. However, since it trains **K separate models** for **K classes**, it can be computationally inefficient for a large number of classes.  \n",
        "\n",
        "**Softmax Regression (Multinomial)**, on the other hand, directly assigns probabilities to all classes in **a single model**, making it more computationally efficient for **large datasets** with multiple classes. It ensures that the sum of probabilities across all classes is **equal to 1**, leading to a more **globally optimized decision boundary**. Softmax works best with solvers like **lbfgs or saga**, which are designed for multiclass problems. However, it assumes that classes are **mutually exclusive**, so it may not work well if the classes overlap significantly.  \n",
        "\n",
        "In summary, **OvR is preferred for small datasets or when using solvers like liblinear, while Softmax is better for large datasets and when a globally optimized probability distribution is needed**. If unsure, **Softmax (Multinomial) with lbfgs** is generally a good choice for most multiclass problems.\n",
        "\n",
        "**20. How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "Ans- In **Logistic Regression**, the coefficients represent the **log-odds** of the outcome variable changing with a one-unit increase in the predictor variable, assuming all other variables remain constant. Mathematically, for a coefficient **β**, the odds of the event occurring change by a factor of **e^β** for every one-unit increase in the corresponding feature. A **positive coefficient** means an increase in that feature increases the probability of the event, while a **negative coefficient** decreases the probability. If the coefficient is **zero**, the feature has no effect. In practice, interpreting coefficients often involves exponentiating them (**e^β**) to understand their impact in terms of odds ratios."
      ],
      "metadata": {
        "id": "9XhAg4MG8izx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PRACTICAL\n",
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using Iris dataset as an example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "spML30F_wJKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization (Lasso)\n",
        "# Note: 'liblinear' solver supports L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PB4_TAgwqwM",
        "outputId": "041d93ed-fa59-4898-8b9d-8558b546343a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dNaYtDxw-bk",
        "outputId": "9780d3df-f31c-4e8e-e4f2-1a9bdcb20c92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9737\n",
            "Model Coefficients:\n",
            "[[-0.43190368 -0.38732553 -0.39343248 -0.46521006 -0.07166728  0.54016395\n",
            "  -0.8014581  -1.11980408  0.23611852  0.07592093 -1.26817815  0.18887738\n",
            "  -0.61058302 -0.9071857  -0.31330675  0.68249145  0.17527452 -0.3112999\n",
            "   0.50042502  0.61622993 -0.87984024 -1.35060559 -0.58945273 -0.84184594\n",
            "  -0.54416967  0.01611019 -0.94305313 -0.77821726 -1.20820031 -0.15741387]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset and split\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CU8MTdBxL8d",
        "outputId": "9b6f1ee7-b076-4fd2-a59a-24965830bad3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9824561403508771\n",
            "Coefficients: [[-4.16702493e-01 -4.57978162e-01 -3.49458606e-01 -4.30139763e-01\n",
            "  -4.37735576e-03  3.90511665e-01 -3.57037020e-01 -7.24779759e-01\n",
            "  -1.21908056e-03  8.11930830e-02 -1.25491541e+00  1.41315344e-01\n",
            "  -3.44819487e-01 -9.78209123e-01 -7.72278541e-02  6.88302791e-01\n",
            "   0.00000000e+00 -3.28292347e-01  2.31606087e-01  3.00233538e-01\n",
            "  -1.08220951e+00 -1.23544618e+00 -7.86891115e-01 -1.05345411e+00\n",
            "  -8.52972172e-01  0.00000000e+00 -6.63448890e-01 -1.14179958e+00\n",
            "  -9.07854740e-01  0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression for multiclass classification using One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and print the model accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48qA9QRIxd1N",
        "outputId": "3e69d200-6905-4606-8556-97771412e752"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8333333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define hyperparameter grid for C and penalty\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression with solver that supports both l1 and l2\n",
        "lr = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and evaluate on test set\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "2eZMDymKxrOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "\n",
        "# Setup Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate model using cross_val_score\n",
        "accuracies = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the average accuracy\n",
        "print(\"Average Accuracy:\", np.mean(accuracies))\n"
      ],
      "metadata": {
        "id": "27221zdPx3sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file (replace 'data.csv' with your file path)\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Assume that the last column is the target and the remaining columns are features\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features to improve model performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "v8f-wH5Cx_dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data and split\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define model and hyperparameter distribution\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "accuracy = accuracy_score(y_test, random_search.best_estimator_.predict(X_test))\n",
        "print(\"Test Set Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "dxs9cCR9yJ8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Logistic Regression and wrap it in OneVsOneClassifier for OvO multiclass classification\n",
        "lr = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "ovo_classifier = OneVsOneClassifier(lr)\n",
        "\n",
        "# Train the OvO classifier\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"OvO Multiclass Logistic Regression Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "-tPQKeV5ydpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GDwMIKM2yqI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Precision, Recall, and F1-Score\n",
        "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "Szv9Dfkjy6XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, n_clusters_per_class=1,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (with stratification)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with class weights to handle imbalance\n",
        "model = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using a classification report and confusion matrix\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7q6e8FUezChM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load Titanic dataset (assumes titanic.csv is available in the working directory)\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Select relevant features and target variable\n",
        "# We'll use: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked; target: Survived\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "data = data[features + ['Survived']]\n",
        "\n",
        "# Handle missing values:\n",
        "# Fill missing Age with the median value and missing Embarked with the mode.\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert categorical variables into dummy/indicator variables\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the data into training (80%) and testing (20%) sets with stratification on target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features for improved model performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data and evaluate performance\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "1D-QO5B8zdY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "#  Without Feature Scaling\n",
        "model_no_scaling = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "\n",
        "# With Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_scaling)\n"
      ],
      "metadata": {
        "id": "DCdeO427zpDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the performance using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "id": "dzvy_NZ9z0sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with a custom learning rate parameter (C=0.5)\n",
        "# Note: In scikit-learn, C is the inverse of regularization strength, which indirectly affects the learning rate.\n",
        "model = LogisticRegression(C=0.5, solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "tT7f0O-R0FLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset and extract features\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize the features for improved performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Extract coefficients (for binary classification, model.coef_ is a 2D array with one row)\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort features by the absolute value of their coefficients\n",
        "feature_importance.sort_values(by='Absolute Coefficient', ascending=False, inplace=True)\n",
        "\n",
        "# Print the sorted feature importance\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "id": "Nn8RfBYN0O6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa)\n"
      ],
      "metadata": {
        "id": "8FO9UjvK0mdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Visualize the Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label='Logistic Regression')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AU3o06Fb0vQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Evaluate and compare the accuracy for each solver\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, penalty='l2', max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with solver '{solver}': {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "I7DR6dCr09w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n"
      ],
      "metadata": {
        "id": "8G_vCf5_1NHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Accuracy on raw data:\", accuracy_raw)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy on standardized data:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "id": "9C1r2NVJ1WDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "\n",
        "# Define grid of values for C (regularization strength)\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Apply GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Retrieve the best parameter and model\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate performance on the test set\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Optimal C (regularization strength):\", best_C)\n",
        "print(\"Test set accuracy with optimal C:\", accuracy)\n"
      ],
      "metadata": {
        "id": "iKoi67gi1elL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import joblib\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model and scaler using joblib\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "\n",
        "# Later, load the model and scaler to make predictions\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "loaded_scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "# Transform test data using the loaded scaler and predict using the loaded model\n",
        "X_test_loaded = loaded_scaler.transform(X_test)\n",
        "y_pred = loaded_model.predict(X_test_loaded)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy with loaded model:\", accuracy)\n"
      ],
      "metadata": {
        "id": "ISp8fqmf1y80"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}