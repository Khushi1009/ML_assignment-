{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4tnbIaOwOOwtJlkE9ctQg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushi1009/ML_assignment-/blob/main/ML_Module10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rn3xR0_qcp5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting Techniques Theoretical**\n",
        "\n",
        "\n",
        "**1.What is Boosting in Machine Learning?**\n",
        "\n",
        "Ans- Boosting is a powerful ensemble learning technique in machine learning that improves the accuracy of predictive models by combining multiple weak learners, typically decision trees, in a sequential manner. The core idea behind boosting is to train models iteratively, where each new model focuses on correcting the errors made by the previous ones. Initially, a weak model is trained on the dataset, and the misclassified instances are given higher weights to make them more influential in the next iteration. This process continues, with each new model learning from the mistakes of its predecessors, thereby reducing errors and improving overall performance. Boosting algorithms, such as AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost, are widely used for classification and regression tasks due to their ability to reduce bias and variance, leading to better generalization. However, boosting can be computationally expensive and prone to overfitting if not tuned properly. Despite these challenges, it remains a popular choice in machine learning competitions and real-world applications where high accuracy is required.\n",
        "\n",
        "**2. How does Boosting differ from Bagging?**\n",
        "\n",
        "Ans-\n",
        "Differences Between Bagging and Boosting:\n",
        "\n",
        "Sequential vs. Parallel:\n",
        "\n",
        "- Bagging: - The base learners are trained independently in parallel, as each learner works on a different subset of the data. The final prediction is typically an average or vote of all base learners.\n",
        "- Boosting: The base learners are trained sequentially, and each learner focuses on correcting the mistakes of its predecessors. The final prediction is a weighted sum of the individual learner predictions.\n",
        "\n",
        "Data Sampling:\n",
        "\n",
        "- Bagging: Utilizes bootstrapping to create multiple subsets of the training data, allowing for variations in the training sets for each base learner.\n",
        "\n",
        "- Boosting: Assigns weights to instances in the training set, with higher weights given to misclassified instances to guide subsequent learners.\n",
        "\n",
        "Weighting of Base Learners:\n",
        "\n",
        "- Bagging: All base learners typically have equal weight when making the final prediction.\n",
        "- Boosting: Assigns different weights to each base learner based on its performance, giving more influence to learners that perform well on challenging instances.\n",
        "\n",
        "Handling Noisy Data and Outliers:\n",
        "- Bagging: Robust to noisy data and outliers due to the averaging or voting mechanism, which reduces the impact of individual errors.\n",
        "- Boosting: More sensitive to noisy data and outliers, as the focus on misclassified instances might lead to overfitting on these instances.\n",
        "\n",
        "Model Diversity:\n",
        "- Bagging: Aims to create diverse base learners through random subsets of the data and, in the case of Random Forests, random feature selection for each tree.\n",
        "- Boosting: Focuses on improving the performance of weak learners sequentially, with each learner addressing the weaknesses of its predecessors.\n",
        "\n",
        "Bias and Variance:\n",
        "- Bagging: Primarily reduces variance by averaging predictions from multiple models, making it effective for models with high variance.\n",
        "- Boosting: Addresses both bias and variance, with a focus on reducing bias by sequentially correcting mistakes made by weak learners.\n",
        "\n",
        "**3. What is the key idea behind AdaBoost?**\n",
        "\n",
        "Ans-\n",
        "The key idea behind **AdaBoost (Adaptive Boosting)** is to improve the accuracy of a weak learning model by focusing more on the difficult-to-classify instances. Instead of training a single strong model, AdaBoost sequentially trains multiple weak learners (usually simple decision trees called decision stumps), where each new model corrects the mistakes of the previous one. By giving more importance to misclassified data points in each iteration, AdaBoost ensures that the next weak model pays special attention to the harder cases, thereby improving overall classification performance.  \n",
        "\n",
        "The process starts by assigning **equal weights** to all training instances. A weak learner is then trained, and its performance is evaluated. If a sample is misclassified, its weight is increased so that it gets more attention in the next iteration. The next weak learner is then trained on this updated dataset, where more focus is given to previously misclassified points. This cycle continues for several iterations, with each weak learner improving upon the mistakes of its predecessors. Finally, the predictions from all weak models are combined in a weighted manner, where more accurate models are given higher importance in the final decision.  \n",
        "\n",
        "AdaBoost is powerful because it transforms a group of weak learners into a **strong classifier**. However, it is sensitive to noisy data and outliers, as misclassified points receive higher weights, which can sometimes lead to overfitting. Despite this, AdaBoost remains a popular boosting technique, serving as the foundation for more advanced models like **Gradient Boosting, XGBoost, and LightGBM**, which further enhance boosting efficiency and performance.\n",
        "\n",
        "\n",
        "\n",
        "**5. What is Gradient Boosting, and how is it different from AdaBoost?**\n",
        "\n",
        "Ans- Gradient Boosting\n",
        "\n",
        "Gradient Boosting is an ensemble learning technique that builds a strong predictive model by combining multiple weak learners (usually decision trees). Unlike AdaBoost, which adjusts sample weights, Gradient Boosting works by minimizing the loss function using gradient descent. Each new model is trained to correct the errors (residuals) of the previous model by learning the gradient of the loss function.\n",
        "\n",
        "\n",
        "Gradient boosting, including algorithms like XGBoost, differs from AdaBoost primarily in how it addresses the errors of previous models: Gradient boosting minimizes errors by optimizing a loss function, while AdaBoost focuses on misclassified data points by re-weighting them.\n",
        "\n",
        "Here's a more detailed comparison:\n",
        "\n",
        "AdaBoost:\n",
        "\n",
        "- Focus: AdaBoost focuses on misclassified data points in each iteration, assigning higher weights to them so that subsequent models focus on these difficult examples.\n",
        "- Reweighting: It re-weights the training samples based on the performance of previous learners.\n",
        "\n",
        "- Weak Learners: AdaBoost can use any algorithm that produces a binary output as a weak learner.\n",
        "-  Simplicity: AdaBoost is known for its simplicity and ease of implementation.\n",
        "\n",
        "- Loss Function: AdaBoost uses a specific loss function (exponential loss).\n",
        "\n",
        "**6. What is the loss function in Gradient Boosting?**\n",
        "\n",
        "Ans- Loss Function in Gradient Boosting\n",
        "In Gradient Boosting, the loss function measures the difference between the actual values and the predicted values. The algorithm minimizes this loss function step by step using gradient descent, where each new weak learner (usually a decision tree) is trained to correct the residual errors of the previous model.\n",
        "\n",
        "**7. How does XGBoost improve over traditional Gradient Boosting?**\n",
        "\n",
        "Ans-\n",
        "\n",
        "XGBoost vs. Traditional Gradient Boosting\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is an improved version of traditional Gradient Boosting that is faster, more efficient, and less prone to overfitting. It introduces several enhancements that make it superior:\n",
        "\n",
        "- Regularization: Unlike traditional Gradient Boosting, XGBoost includes L1 (Lasso) and L2 (Ridge) regularization, preventing overfitting.\n",
        "\n",
        "- Faster Tree Splitting: It uses a quantile-based algorithm instead of sorting data at each split, making training much faster.\n",
        "\n",
        "- Parallel Processing: While traditional Gradient Boosting builds trees sequentially, XGBoost can train multiple splits in parallel, speeding up computation.\n",
        "- Automatic Missing Value Handling: XGBoost learns the best way to handle missing values during training, reducing preprocessing effort.\n",
        "- Efficient Tree Pruning: Instead of growing trees to a fixed size, XGBoost prunes trees dynamically to keep only meaningful splits.\n",
        "- Better Handling of Imbalanced Data: The scale_pos_weight parameter helps balance classes, making it more effective for fraud detection and rare event prediction.\n",
        "- GPU Support: Unlike traditional Gradient Boosting, XGBoost supports GPU acceleration, significantly boosting training speed for large datasets.\n",
        "\n",
        "**What is the difference between XGBoost and CatBoost?**\n",
        "\n",
        "Ans\n",
        "\n",
        "XGBoost vs. CatBoost: Key Differences\n",
        "XGBoost (Extreme Gradient Boosting) and CatBoost (Categorical Boosting) are both powerful Gradient Boosting algorithms, but they differ in how they handle categorical data, speed, and overall efficiency. While XGBoost is widely used for structured data, CatBoost is specifically optimized for datasets with categorical features.\n",
        "\n",
        "One of the biggest differences between the two is how they handle categorical data. XGBoost requires categorical variables to be manually converted into numerical values using One-Hot Encoding or Label Encoding before training. This preprocessing step can increase memory usage and sometimes lead to suboptimal splits. In contrast, CatBoost has a built-in method for handling categorical features called Ordered Target Encoding, which learns patterns more effectively while reducing overfitting. This makes CatBoost particularly useful for datasets with many categorical variables.\n",
        "\n",
        "In terms of training speed and efficiency, XGBoost is optimized for performance and supports parallel processing and GPU acceleration, making it significantly faster than traditional Gradient Boosting methods. However, CatBoost uses an Ordered Boosting algorithm, which ensures better generalization and reduces the risk of overfitting, especially when training on large datasets. In many cases, CatBoost can be faster and more memory-efficient than XGBoost, particularly when working with mixed data types.\n",
        "\n",
        "When comparing their performance on different types of datasets, XGBoost is well-suited for numerical datasets and structured tabular data, making it a popular choice in machine learning competitions and real-world applications like finance and healthcare. CatBoost, on the other hand, excels in tasks involving a large number of categorical features, such as customer behavior analysis and recommendation systems. Additionally, CatBoost often requires fewer hyperparameter adjustments, making it easier to use for beginners.\n",
        "\n",
        "**9. What are some real-world applications of Boosting techniques?**\n",
        "\n",
        "ANs- Real-World Applications of Boosting Techniques\n",
        "Boosting algorithms, such as AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost, are widely used across various industries due to their high accuracy and ability to handle complex data. Here are some key real-world applications of Boosting techniques:\n",
        "\n",
        "1. Fraud Detection\n",
        "Boosting algorithms are extensively used in fraud detection for banking, e-commerce, and insurance. They help identify fraudulent transactions by learning subtle patterns from large datasets. XGBoost and LightGBM are particularly effective in credit card fraud detection, where data imbalance (few fraud cases compared to normal transactions) is a challenge.\n",
        "\n",
        " Example: Banks use XGBoost to detect anomalies in credit card transactions and flag potential fraud.\n",
        "\n",
        "2. Healthcare & Disease Prediction\n",
        "Boosting techniques play a crucial role in medical diagnosis and predictive analytics. They help in predicting diseases based on patient records, lab results, and genetic data. Gradient Boosting and CatBoost are widely used in cancer detection, heart disease prediction, and drug discovery.\n",
        "\n",
        " Example: Hospitals use Gradient Boosting models to predict patient readmission risks based on medical history and treatment patterns.\n",
        "\n",
        "3. Recommendation Systems\n",
        "E-commerce platforms and streaming services use Boosting to provide personalized recommendations. These models analyze user behavior, purchase history, and preferences to suggest relevant products, movies, or songs. CatBoost and XGBoost are commonly used for improving recommendation accuracy.\n",
        "\n",
        " Example: Netflix and Amazon use boosting models to recommend movies and products based on user behavior.\n",
        "\n",
        "4. Finance & Risk Assessment\n",
        "Boosting is heavily used in credit scoring, stock market prediction, and risk assessment. Financial institutions leverage Gradient Boosting and XGBoost to assess loan applicants' creditworthiness and predict stock price movements.\n",
        "\n",
        " Example: Banks use XGBoost to evaluate loan applicants by analyzing their credit history, income, and financial behavior.\n",
        "\n",
        "5. Customer Churn Prediction\n",
        "Boosting models help businesses predict which customers are likely to leave (churn) based on past behavior. Companies use this insight to retain customers through targeted marketing and personalized offers.\n",
        "\n",
        " Example: Telecom companies use CatBoost to predict which users might switch to competitors and offer them better plans.\n",
        "\n",
        "6. Image Recognition & Computer Vision\n",
        "Boosting techniques are applied in image classification, face recognition, and object detection. While deep learning dominates this space, boosting models are useful when training data is limited or for feature engineering before deep learning.\n",
        "\n",
        " Example: Boosting is used in facial recognition systems for security and biometric authentication.\n",
        "\n",
        "7. Natural Language Processing (NLP)\n",
        "Boosting is widely used in sentiment analysis, spam detection, and text classification. XGBoost and LightGBM help classify emails as spam or predict user sentiment based on customer reviews.\n",
        "\n",
        " Example: Gmail uses Gradient Boosting to filter spam emails and categorize them into primary, social, and promotions tabs.\n",
        "\n",
        "8. Autonomous Vehicles & Predictive Maintenance\n",
        "Boosting models help in predicting equipment failures and optimizing self-driving car systems. These models analyze sensor data to detect potential failures before they occur.\n",
        "\n",
        " Example: Self-driving cars use Gradient Boosting to detect obstacles and predict vehicle movements.\n",
        "\n",
        "\n",
        "**10. How does regularization help in XGBoost?**\n",
        "\n",
        "Ans-\n",
        "\n",
        "Regularization in **XGBoost (Extreme Gradient Boosting)** helps prevent **overfitting** by penalizing overly complex models. Unlike traditional Gradient Boosting, which can create deep and highly complex trees that memorize the training data, XGBoost introduces **L1 (Lasso) and L2 (Ridge) regularization** to encourage simpler models that generalize better to new data.  \n",
        "\n",
        "**L1 regularization (Lasso)** applies a penalty on the **absolute values of tree leaf weights**, forcing some of them to become zero. This acts as an automatic **feature selection mechanism**, removing irrelevant or redundant features from the model. As a result, it simplifies the model and improves interpretability while maintaining accuracy.  \n",
        "\n",
        "**L2 regularization (Ridge)**, on the other hand, penalizes the **squared values of tree leaf weights**, preventing any single feature from dominating the predictions. This helps distribute the importance more evenly among the features, reducing the risk of **overfitting to noise** in the training data.  \n",
        "\n",
        "By combining both L1 and L2 regularization, XGBoost ensures that the model remains **balanced—complex enough to learn patterns effectively but simple enough to generalize well**. This makes it one of the most powerful and reliable boosting algorithms for structured data problems.\n",
        "\n",
        "**11. What are some hyperparameters to tune in Gradient Boosting models?**\n",
        "\n",
        "Ans- **Hyperparameters to Tune in Gradient Boosting Models**  \n",
        "\n",
        "Tuning hyperparameters is crucial for optimizing **Gradient Boosting models** (including **XGBoost, LightGBM, and CatBoost**) to achieve the best performance. Here are some key hyperparameters and their effects:  \n",
        "\n",
        "---\n",
        "\n",
        " **1. Learning Rate (η)**\n",
        "- Controls how much the model **adjusts** with each new tree.  \n",
        "- **Lower values (0.01 - 0.1):** More accurate but requires more trees.  \n",
        "- **Higher values (0.2 - 0.3+):** Faster training but risks **overfitting**.  \n",
        "\n",
        " **Tip:** A small learning rate with more trees usually works best.  \n",
        "\n",
        "---\n",
        "\n",
        " **2. Number of Estimators (n_estimators)**\n",
        "- Defines the **number of trees** in the model.  \n",
        "- Too **low** → Model underfits.  \n",
        "- Too **high** → Model overfits (especially if learning rate is high).  \n",
        "\n",
        " **Tip:** Use **early stopping** to find the optimal number.  \n",
        "\n",
        "---\n",
        "\n",
        " **3. Maximum Depth of Trees (max_depth)**\n",
        "- Controls the **complexity** of each tree.  \n",
        "- **Shallow trees (2-6):** Prevent overfitting but may underfit.  \n",
        "- **Deep trees (7-15+):** Capture more patterns but risk overfitting.  \n",
        "\n",
        " **Tip:** Use **max_depth = 4-8** for tabular data.  \n",
        "\n",
        "---\n",
        "\n",
        " **4. Minimum Child Weight (min_child_weight)**\n",
        "- Defines the **minimum number of instances in a leaf node**.  \n",
        "- **Higher values (3-10):** More conservative, prevents overfitting.  \n",
        "- **Lower values (1-3):** More flexible but risks overfitting.  \n",
        "\n",
        " **Tip:** Start with **min_child_weight = 1** and increase if overfitting occurs.  \n",
        "\n",
        "---\n",
        "\n",
        " **5. Subsample (subsample)**\n",
        "- Determines what **fraction of data** is used for each boosting step.  \n",
        "- **Lower values (0.5-0.8):** Reduces overfitting, improves generalization.  \n",
        "- **Higher values (0.9-1.0):** Uses more data, but risks overfitting.  \n",
        "\n",
        " **Tip:** Use **0.6-0.8** to reduce overfitting.  \n",
        "\n",
        "---\n",
        "\n",
        " **6. Column Subsampling (colsample_bytree, colsample_bylevel, colsample_bynode)**\n",
        "- Controls how many **features** are used per tree.  \n",
        "- **colsample_bytree (0.5-0.9):** Fraction of features per tree.  \n",
        "- **colsample_bylevel:** Fraction of features per tree level.  \n",
        "- **colsample_bynode:** Fraction per node split.  \n",
        "\n",
        " **Tip:** Use **0.6-0.8** for large feature sets.  \n",
        "\n",
        "---\n",
        "**7. Gamma (γ) – Minimum Loss Reduction**\n",
        "- Controls **when to split** a node.  \n",
        "- **Higher values (1-10):** More conservative, prevents overfitting.  \n",
        "- **Lower values (0-1):** More aggressive, allows deeper trees.  \n",
        "\n",
        " **Tip:** Start with **γ = 0** and increase if overfitting occurs.  \n",
        "\n",
        "---\n",
        "**8. L1 & L2 Regularization (alpha & lambda)**\n",
        "- **L1 (alpha):** Makes the model **sparser**, removing unnecessary splits.  \n",
        "- **L2 (lambda):** Helps prevent large leaf values, reducing overfitting.  \n",
        "\n",
        " **Tip:** Use **lambda = 1** (default) and tune **alpha (0-5)** for better feature selection.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**12. What is the concept of Feature Importance in Boosting**\n",
        "\n",
        "Ans-\n",
        " **Concept of Feature Importance in Boosting**  \n",
        "\n",
        "**Feature Importance** in Boosting refers to the ranking of input variables based on how much they contribute to the model’s predictions. In Boosting algorithms like **XGBoost, LightGBM, and CatBoost**, decision trees are built sequentially, and the importance of each feature is measured by how often it is used for splitting and how much it reduces the **loss function (error).**  \n",
        "\n",
        "There are different ways to calculate feature importance in Boosting models: **Gain, Frequency, and Cover.** **Gain** measures how much a feature improves splits, **Frequency** counts how often a feature is used in trees, and **Cover** looks at the number of samples affected by a split. These insights help in **feature selection**, improving model interpretability, and reducing dimensionality by removing less significant features. Feature importance visualization using **SHAP (Shapley Additive Explanations)** or built-in importance plots in XGBoost and LightGBM provides deeper insights into which factors drive predictions, making Boosting models not only powerful but also explainable.\n",
        "\n",
        "**13. Why is CatBoost efficient for categorical data??**\n",
        "\n",
        "Ans-\n",
        "\n",
        "\n",
        "CatBoost is highly efficient for categorical data because it has a **built-in mechanism** for handling categorical features without requiring manual encoding. Unlike other boosting algorithms like XGBoost and LightGBM, which require **One-Hot Encoding** or **Label Encoding**, CatBoost uses an advanced technique called **Ordered Target Encoding** and **Permutation-based Algorithm**, which improves both accuracy and efficiency.  \n",
        "\n",
        "1. **Ordered Target Encoding:** Instead of using traditional label encoding (which can cause data leakage), CatBoost **dynamically encodes categorical variables** by considering the order of data points, ensuring better generalization.  \n",
        "\n",
        "2. **Efficient GPU & CPU Optimization:** CatBoost is optimized for both CPU and GPU processing, making it faster than other boosting methods when working with large categorical datasets.  \n",
        "\n",
        "3. **No Need for Extensive Preprocessing:** Since CatBoost automatically handles categorical variables, it **reduces preprocessing time** and eliminates the risk of choosing the wrong encoding strategy.  \n",
        "\n",
        "4. **Better Performance on Imbalanced Data:** The way CatBoost processes categorical features helps in handling **imbalanced datasets** more effectively, reducing bias in predictions.  \n",
        "\n"
      ],
      "metadata": {
        "id": "UMElKK-6q4po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PRACTICAL\n",
        "\n",
        "#14. Train an AdaBoost Classifier on a sample dataset and print model accuracy.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost with a Decision Tree base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)  # Weak learner (stump)\n",
        "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Train the AdaBoost model\n",
        "ada_boost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ada_boost.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CgB7M_AofkKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE).\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost Regressor with a Decision Tree base estimator\n",
        "base_regressor = DecisionTreeRegressor(max_depth=3)  # Weak learner\n",
        "ada_boost_regressor = AdaBoostRegressor(base_estimator=base_regressor, n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Train the AdaBoost Regressor model\n",
        "ada_boost_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ada_boost_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print MAE\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "NMp4FON-qnAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = gb_classifier.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': cancer.feature_names,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importance table\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance in Gradient Boosting Classifier\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YMfgrvWIrcoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate R-Squared Score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print R-Squared Score\n",
        "print(f\"R-Squared Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "muTxi1-rsDvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Initialize and train the XGBoost Classifier\n",
        "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_gb = gb_classifier.predict(X_test)\n",
        "y_pred_xgb = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy_gb:.2f}\")\n",
        "print(f\"XGBoost Accuracy: {accuracy_xgb:.2f}\")\n"
      ],
      "metadata": {
        "id": "CAbtYJ4asYlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Train a CatBoost Classifier and evaluate using F1-Score\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "catboost_classifier = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "catboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = catboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print F1-Score\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "x3okOEFDsu4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the XGBoost Regressor with optimized parameters\n",
        "xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, verbosity=1, random_state=42)\n",
        "xgb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print MSE\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "az1gDKpBtK4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train an AdaBoost Classifier and visualize feature importance\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the AdaBoost Classifier with Decision Tree as the base estimator\n",
        "adaboost_classifier = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = adaboost_classifier.feature_importances_\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, feature_importance, color='skyblue')\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance in AdaBoost Classifier\")\n",
        "plt.gca().invert_yaxis()  # Invert axis to show the most important feature at the top\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "23XC6kwZt22G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a Gradient Boosting Regressor and plot learning curves.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Store training and validation loss for learning curve\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "# Train model and record loss at each stage\n",
        "for n in range(1, 101):  # 1 to 100 estimators\n",
        "    gb_regressor.n_estimators = n\n",
        "    gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Compute training and validation errors\n",
        "    y_train_pred = gb_regressor.predict(X_train)\n",
        "    y_val_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
        "    val_errors.append(mean_squared_error(y_test, y_val_pred))\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 101), train_errors, label=\"Training Loss\", marker='o')\n",
        "plt.plot(range(1, 101), val_errors, label=\"Validation Loss\", marker='s')\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Gradient Boosting Regressor Learning Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XoGeeQWxuYl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train an XGBoost Classifier and visualize feature importance\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the XGBoost Classifier\n",
        "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"XGBoost Classifier Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_importance(xgb_classifier, importance_type=\"weight\", xlabel=\"Feature Importance Score\")\n",
        "plt.title(\"Feature Importance in XGBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WDp-n9D4us1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a CatBoost Classifier and plot the confusion matrix\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "catboost_classifier = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "catboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = catboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"CatBoost Classifier Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Malignant\", \"Benign\"], yticklabels=[\"Malignant\", \"Benign\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix for CatBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C7OYj7A7u-B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different numbers of estimators to test\n",
        "n_estimators_list = [10, 50, 100, 200, 300]\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train AdaBoost Classifier with different numbers of estimators\n",
        "for n in n_estimators_list:\n",
        "    ada_classifier = AdaBoostClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "        n_estimators=n,\n",
        "        learning_rate=1.0,\n",
        "        random_state=42\n",
        "    )\n",
        "    ada_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = ada_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Accuracy with {n} estimators: {accuracy:.2f}\")\n",
        "\n",
        "# Plot the accuracy vs. number of estimators\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, accuracy_scores, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs. Number of Estimators in AdaBoost Classifier\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ny3a8-zfvVXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Train a Gradient Boosting Classifier and visualize the ROC curve\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probability scores\n",
        "y_scores = gb_classifier.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Gradient Boosting Classifier\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yZEt_WhfvuUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "xgb_regressor = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for tuning the learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]  # Testing different learning rates\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(xgb_regressor, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best learning rate\n",
        "best_learning_rate = grid_search.best_params_['learning_rate']\n",
        "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
        "\n",
        "# Train XGBoost Regressor with the best learning rate\n",
        "best_xgb = xgb.XGBRegressor(n_estimators=100, learning_rate=best_learning_rate, max_depth=3, random_state=42)\n",
        "best_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate model\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error with Best Learning Rate: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "kuxykDOIv9DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier without class weights\n",
        "catboost_no_weights = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "catboost_no_weights.fit(X_train, y_train)\n",
        "\n",
        "# Train CatBoost Classifier with class weights\n",
        "class_weights = {0: 1, 1: 9}  # Adjusting weights to balance class impact\n",
        "catboost_with_weights = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, class_weights=class_weights, verbose=0, random_state=42)\n",
        "catboost_with_weights.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_no_weights = catboost_no_weights.predict(X_test)\n",
        "y_pred_with_weights = catboost_with_weights.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Without Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "print(\"\\nWith Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n",
        "\n",
        "# Compute confusion matrices\n",
        "cm_no_weights = confusion_matrix(y_test, y_pred_no_weights)\n",
        "cm_with_weights = confusion_matrix(y_test, y_pred_with_weights)\n",
        "\n",
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sns.heatmap(cm_no_weights, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"], ax=axes[0])\n",
        "axes[0].set_title(\"Without Class Weights\")\n",
        "axes[0].set_xlabel(\"Predicted Label\")\n",
        "axes[0].set_ylabel(\"True Label\")\n",
        "\n",
        "sns.heatmap(cm_with_weights, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"], ax=axes[1])\n",
        "axes[1].set_title(\"With Class Weights\")\n",
        "axes[1].set_xlabel(\"Predicted Label\")\n",
        "axes[1].set_ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hMCVjCmowgpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29.Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different learning rates to test\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train AdaBoost Classifier with different learning rates\n",
        "for lr in learning_rates:\n",
        "    ada_classifier = AdaBoostClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "        n_estimators=100,\n",
        "        learning_rate=lr,\n",
        "        random_state=42\n",
        "    )\n",
        "    ada_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = ada_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Accuracy with learning rate {lr}: {accuracy:.4f}\")\n",
        "\n",
        "# Plot the accuracy vs. learning rate\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(learning_rates, accuracy_scores, marker='o', linestyle='--', color='b')\n",
        "plt.xscale(\"log\")  # Log scale for better visualization\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Learning Rate on AdaBoost Classifier Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ah6IZM40w-A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Digits dataset (multi-class classification problem)\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize XGBoost Classifier for multi-class classification\n",
        "xgb_classifier = xgb.XGBClassifier(\n",
        "    objective=\"multi:softprob\",  # Required for multi-class classification\n",
        "    num_class=len(np.unique(y)),  # Number of classes\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probability distributions for each class\n",
        "y_prob = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Compute log-loss\n",
        "loss = log_loss(y_test, y_prob)\n",
        "print(f\"Log-Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "m5wH8KuWxewA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}